{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysesUsingKeras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tarun-jethwani/SentimentAnalyses/blob/master/SentimentAnalysesUsingKeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAubf2-rHZXG",
        "colab_type": "text"
      },
      "source": [
        "### I have kept the Dataset inside Google Drive, so now It is necessary to mount my gdrive here, to read the dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "992NciVMHsL1",
        "colab_type": "code",
        "outputId": "f32a7121-bcd6-41c9-91be-470f45f5b200",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po_cCKv7HsLL",
        "colab_type": "code",
        "outputId": "6639b364-bbea-4dff-f439-6b2b0f8d483e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "with open('/gdrive/My Drive/foo.txt', 'w') as f:\n",
        "  f.write('Hello Google Drive!')\n",
        "!cat '/gdrive/My Drive/foo.txt'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello Google Drive!"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePY-15CoIMDC",
        "colab_type": "text"
      },
      "source": [
        "# Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD6srNiLITNw",
        "colab_type": "code",
        "outputId": "182e12e8-e8b3-404d-f1b3-5f551b36d215",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import *\n",
        "from keras.models import Sequential\n",
        "import keras\n",
        "from keras import backend as K "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXJiUdeOIIFa",
        "colab_type": "text"
      },
      "source": [
        "### Read the Dataset from CSV file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_GRB5oVIG8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = pd.read_csv('/gdrive/My Drive/kuc-hackathon-winter-2018/drugsComTrain_raw.csv')\n",
        "test_data = pd.read_csv('/gdrive/My Drive/kuc-hackathon-winter-2018/drugsComTest_raw.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU6GwkqkJKMb",
        "colab_type": "code",
        "outputId": "c6f2d237-cb3d-4ef2-a6c7-89074707d433",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uniqueID</th>\n",
              "      <th>drugName</th>\n",
              "      <th>condition</th>\n",
              "      <th>review</th>\n",
              "      <th>rating</th>\n",
              "      <th>date</th>\n",
              "      <th>usefulCount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>206461</td>\n",
              "      <td>Valsartan</td>\n",
              "      <td>Left Ventricular Dysfunction</td>\n",
              "      <td>\"It has no side effect, I take it in combinati...</td>\n",
              "      <td>9</td>\n",
              "      <td>20-May-12</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>95260</td>\n",
              "      <td>Guanfacine</td>\n",
              "      <td>ADHD</td>\n",
              "      <td>\"My son is halfway through his fourth week of ...</td>\n",
              "      <td>8</td>\n",
              "      <td>27-Apr-10</td>\n",
              "      <td>192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>92703</td>\n",
              "      <td>Lybrel</td>\n",
              "      <td>Birth Control</td>\n",
              "      <td>\"I used to take another oral contraceptive, wh...</td>\n",
              "      <td>5</td>\n",
              "      <td>14-Dec-09</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>138000</td>\n",
              "      <td>Ortho Evra</td>\n",
              "      <td>Birth Control</td>\n",
              "      <td>\"This is my first time using any form of birth...</td>\n",
              "      <td>8</td>\n",
              "      <td>3-Nov-15</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>35696</td>\n",
              "      <td>Buprenorphine / naloxone</td>\n",
              "      <td>Opiate Dependence</td>\n",
              "      <td>\"Suboxone has completely turned my life around...</td>\n",
              "      <td>9</td>\n",
              "      <td>27-Nov-16</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   uniqueID                  drugName  ...       date usefulCount\n",
              "0    206461                 Valsartan  ...  20-May-12          27\n",
              "1     95260                Guanfacine  ...  27-Apr-10         192\n",
              "2     92703                    Lybrel  ...  14-Dec-09          17\n",
              "3    138000                Ortho Evra  ...   3-Nov-15          10\n",
              "4     35696  Buprenorphine / naloxone  ...  27-Nov-16          37\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4sLCU7pKEOP",
        "colab_type": "text"
      },
      "source": [
        "### Drop Duplicates and NA values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wZ25ma5KCmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def drop_duplicatesNA(data):\n",
        "  data.drop_duplicates(inplace=True)  #dropping duplicates\n",
        "  data.dropna(axis=0,inplace=True)\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuS2BgpkLk0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = drop_duplicatesNA(train_data)\n",
        "test_data = drop_duplicatesNA(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf9Xd8-dLE-n",
        "colab_type": "text"
      },
      "source": [
        "### Creating Input and Ouput set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2MdTbnzKs7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = train_data['review'].to_numpy()\n",
        "Y_train = train_data['rating'].to_numpy()\n",
        "X_test = test_data['review'].to_numpy()\n",
        "Y_test = test_data['rating'].to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6aYaS1RuWyR",
        "colab_type": "text"
      },
      "source": [
        "## Filter Review \n",
        "\n",
        "### 1--> Remove Special symbols, punctuation\n",
        "### 2--> Converting the text into lower case\n",
        "### 3--> Removing double quotes(\")\n",
        "### 4--> substitute Contractions\n",
        "### 5--> Remove stopwords('English') barring no, not , nor (because these words might play an important role in guessing the sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8erSpdPtgT3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou6TeHYlqSBC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words.remove('no')\n",
        "stop_words.remove('not')\n",
        "stop_words.remove('nor')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STQqqeWOi-Ki",
        "colab_type": "text"
      },
      "source": [
        "### Contractions Mapping "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cn_uHqPoi6F5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "\n",
        "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9FrJ4t9rHwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_review(X_train):\n",
        "  X = []\n",
        "  for x in X_train:\n",
        "    x = re.sub('[^A-Za-z0-9]+', ' ', x)\n",
        "    x = x.lower()\n",
        "    x = re.sub('\"', '', x)\n",
        "    x = ' '.join([contraction_mapping[w] if w in contraction_mapping else w for w in x.split()])\n",
        "    x = ' '.join([w for w in x.split() if w not in stop_words])\n",
        "    x = re.sub('[^A-Za-z0-9]+', ' ', x)\n",
        "    X.append(x)\n",
        "  return np.array(X)\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRgHM3uXeMdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = filter_review(X_train)\n",
        "X_test = filter_review(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cba4Linwv-21",
        "colab_type": "text"
      },
      "source": [
        "### This is how filtered reviews look like, we are going to use this as an input to train are Sentiment Analyses Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_bpwR61v-EA",
        "colab_type": "code",
        "outputId": "353a040e-431f-4008-fea5-2a8e361a1347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "for i in range(10):\n",
        "  print(i,\" >\", X_train[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0  > no side effect take combination bystolic 5 mg fish oil\n",
            "1  > son halfway fourth week intuniv became concerned began last week started taking highest dose two days could hardly get bed cranky slept nearly 8 hours drive home school vacation unusual called doctor monday morning said stick days see school getting morning last two days problem free much agreeable ever less emotional good thing less cranky remembering things overall behavior better tried many different medications far effective\n",
            "2  > used take another oral contraceptive 21 pill cycle happy light periods max 5 days no side effects contained hormone gestodene not available us switched lybrel ingredients similar pills ended started lybrel immediately first day period instructions said period lasted two weeks taking second pack two weeks third pack things got even worse third period lasted two weeks 039 end third week still daily brown discharge positive side 039 side effects idea period free tempting alas\n",
            "3  > first time using form birth control 039 glad went patch 8 months first decreased libido subsided downside made periods longer 5 6 days exact used periods 3 4 days max also made cramps intense first two days period never cramps using birth control happy patch\n",
            "4  > suboxone completely turned life around feel healthier 039 excelling job always money pocket savings account none suboxone spent years abusing oxycontin paycheck already spent time got started resorting scheming stealing fund addiction history 039 ready stop 039 good chance suboxone put path great life found side effects minimal compared oxycontin 039 actually sleeping better slight constipation truly amazing cost pales comparison spent oxycontin\n",
            "5  > 2nd day 5mg started work rock hard erections however experianced headache lower bowel preassure 3rd day erections would wake amp hurt leg ankles aches severe lower bowel preassure like need go 2 039 enjoyed initial rockhard erections not side effects 230 months supply 039 50 amp work 3xs week not worth side effects\n",
            "6  > pulled cummed bit took plan b 26 hours later took pregnancy test two weeks later 039 pregnant\n",
            "7  > abilify changed life hope zoloft clonidine first started abilify age 15 zoloft depression clondine manage complete rage moods control depressed hopeless one second mean irrational full rage next dr prescribed 2mg abilify point feel like cured though know 039 not bi polar disorder constant battle know abilify works tried get lost complete control emotions went back golden 5mg 2x daily 21 better ever past side effect like eat lot\n",
            "8  > nothing problems keppera constant shaking arms amp legs amp pins amp needles feeling arms amp legs severe light headedness no appetite amp etc\n",
            "9  > pill many years doctor changed rx chateal effective really help completely clearing acne takes 6 months though not gain extra weight develop emotional health issues stopped taking bc started using natural method birth control started take bc hate acne came back age 28 really hope symptoms like depression weight gain not begin affect older 039 also naturally moody may worsen things negative mental rut today also hope 039 push edge believe depressed hopefully 039 like younger\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpqMWf6PwY2a",
        "colab_type": "text"
      },
      "source": [
        "Looks clean, isn't it ??? :>)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAiDw-d8MxbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = len(max(X_train).split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TncBfhPvRq1Z",
        "colab_type": "text"
      },
      "source": [
        "### subtracting 1 from Y_train will give me Rating coherent with number of classes, to match class number with rating number "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeN-8yW8RZdl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train_minus = Y_train - 1\n",
        "Y_test_minus = Y_test - 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB96IJ52SEQK",
        "colab_type": "text"
      },
      "source": [
        "### Convert to One Hot vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Daivj6WuQwXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_one_hot(Y, C):\n",
        "    Y = np.eye(C)[Y.reshape(-1)]\n",
        "    return Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_8LzXXySJqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train_one_hot = convert_to_one_hot(Y_train_minus, 10)\n",
        "Y_test_one_hot = convert_to_one_hot(Y_test_minus,10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwpz9PTGS2R-",
        "colab_type": "text"
      },
      "source": [
        "### Priniting the output shape of the vector to verify dimensions of the transformed vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr9MAZsQSfn-",
        "colab_type": "code",
        "outputId": "4b0a3ad6-3d8f-4234-ff62-1766677859aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "Y_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(160398,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MiVNTEsShVr",
        "colab_type": "code",
        "outputId": "8e3bd772-a417-49a4-d78c-9ed02c151bd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "Y_train_one_hot.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(160398, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMdQdwYDTB9x",
        "colab_type": "text"
      },
      "source": [
        "### Function which reads Glove File, loads 100D vectors and returns words2index and index2words dictionary "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1toSPro1TBLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_glove():\n",
        "  \n",
        "  with open('/gdrive/My Drive/glove.6B/glove.6B.100d.txt', 'r') as f:\n",
        "    words = set()\n",
        "    word2vec = {}\n",
        "    for line in f:\n",
        "      line = line.strip().split()\n",
        "      word = line[0]\n",
        "      words.add(word)\n",
        "      word2vec[word] = np.array(line[1:], dtype= np.float64)\n",
        "      \n",
        "  # Creating word2idx and idx2word dictionaries\n",
        "  i = 0\n",
        "  word2idx = {}\n",
        "  idx2word = {}\n",
        "  for word in sorted(words):\n",
        "    word2idx[word] = i\n",
        "    idx2word[i] = word\n",
        "    i += 1\n",
        "\n",
        "  return word2idx, idx2word, word2vec\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GzrrYu3cEFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2idx, idx2word, word2vec = load_glove()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_EbAYqPmFCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2idx['eos'], word2idx['!'] = word2idx['!'], word2idx['eos']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kfWSALqmwZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx2word = {v:k for k,v in word2idx.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4BzslnxxWD5",
        "colab_type": "text"
      },
      "source": [
        "### Adding words from X_train to word2idx and idx2word, if word not already exist in X_train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB8hd8MAxwxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for sentence in X_train:\n",
        "  for word in word_tokenize(sentence):\n",
        "    if word not in word2idx:\n",
        "      i = len(word2idx) \n",
        "      word2idx[word] = i\n",
        "      idx2word[i] = word      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gU4dV1B47age",
        "colab_type": "text"
      },
      "source": [
        "### Adding words from X_test to word2idx and idx2word, if word not already exist in X_test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHMgyt6s7fnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for sentence in X_test:\n",
        "  for word in word_tokenize(sentence):\n",
        "    if word not in word2idx:\n",
        "      i = len(word2idx) \n",
        "      word2idx[word] = i\n",
        "      idx2word[i] = word  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IjRoBvVdZyg",
        "colab_type": "text"
      },
      "source": [
        "### Creating Embedding Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D47J5lQQdqaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(word2idx)\n",
        "embedding_dim = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9wXUd0BdkFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for i,word in enumerate(word2idx):\n",
        "  try:\n",
        "    embedding_matrix[i] = word2vec[word]\n",
        "  except KeyError:\n",
        "    embedding_matrix[i] = np.random.normal(scale=0.6, size=(100,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFwaoPvejc3H",
        "colab_type": "text"
      },
      "source": [
        "### Prepare Training Set\n",
        "\n",
        "1.   convert indices from sentence\n",
        "2.   convert entire set to set of indices\n",
        "3.   Padding Sequnces with 0 till max_len\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMK4jc7jjcJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def indices_from_sentence(sentence):\n",
        "  return np.array([word2idx[word] for word in word_tokenize(sentence)])\n",
        "\n",
        "def sequence_set(sentences):\n",
        "  sentence_set = []\n",
        "  for sentence in sentences:\n",
        "    sentence_set.append(indices_from_sentence(sentence))\n",
        "  return np.array(sentence_set)\n",
        "    \n",
        "def post_padding(input_set, max_len):\n",
        "  return pad_sequences(input_set,  maxlen=max_len, padding='post')\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoOwWMbmodY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_tr = sequence_set(X_train)\n",
        "X_tr = post_padding(X_tr, max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mgp9zew58kDe"
      },
      "source": [
        "### Now, Building Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PHA2Y9ye8kDi"
      },
      "source": [
        "**-- Prepare the Test Set ( same like we got Train Set)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NGFznkN88kDj",
        "colab": {}
      },
      "source": [
        "X_test = filter_review(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yjswIzCs8kDm",
        "colab": {}
      },
      "source": [
        "X_test_set = sequence_set(X_test)\n",
        "X_test_set = post_padding(X_test_set, max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7flK8aGK1RfE",
        "colab_type": "text"
      },
      "source": [
        "## Defining Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ty5zPzeJH071",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentiment_analyses_model(vocab_size, embedding_dim):\n",
        "  \n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Embedding(vocab_size, embedding_dim, weights = [embedding_matrix], input_length=max_len, trainable=False))\n",
        "    \n",
        "    model.add(LSTM(1024, return_sequences = True))\n",
        "    \n",
        "    model.add(LSTM(1024, return_sequences = True))\n",
        "    \n",
        "    model.add(LSTM(1024))\n",
        "    \n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC0cJ3j54cDH",
        "colab_type": "code",
        "outputId": "9b5097d9-6d0b-4ee1-aa60-5b92027f881f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "model = sentiment_analyses_model(vocab_size, embedding_dim)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPfCnRt38A6q",
        "colab_type": "code",
        "outputId": "5183a4db-b3ab-4a41-ea86-c9a7ba582f8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 28, 100)           42134200  \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 28, 1024)          4608000   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 28, 1024)          8392704   \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 1024)              8392704   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 63,537,858\n",
            "Trainable params: 21,403,658\n",
            "Non-trainable params: 42,134,200\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPbrjVzzbcXn",
        "colab_type": "code",
        "outputId": "25c863b4-4e8a-4ae9-d01a-88dc674f2f9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xiWzI7qIU03",
        "colab_type": "code",
        "outputId": "1623f133-c565-4552-d52a-0fb9c02e8337",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "model.fit(X_tr, Y_train_one_hot, epochs = 10, batch_size = 32, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch 1/10\n",
            "160398/160398 [==============================] - 2075s 13ms/step - loss: 1.8040 - acc: 0.3719\n",
            "Epoch 2/10\n",
            "160398/160398 [==============================] - 2070s 13ms/step - loss: 1.6161 - acc: 0.4226\n",
            "Epoch 3/10\n",
            "160398/160398 [==============================] - 2066s 13ms/step - loss: 1.4392 - acc: 0.4685\n",
            "Epoch 4/10\n",
            "160398/160398 [==============================] - 2079s 13ms/step - loss: 1.1724 - acc: 0.5596\n",
            "Epoch 5/10\n",
            "160398/160398 [==============================] - 2083s 13ms/step - loss: 0.9110 - acc: 0.6585\n",
            "Epoch 6/10\n",
            "160398/160398 [==============================] - 2084s 13ms/step - loss: 0.6953 - acc: 0.7460\n",
            "Epoch 7/10\n",
            "160398/160398 [==============================] - 2076s 13ms/step - loss: 0.5322 - acc: 0.8100\n",
            "Epoch 8/10\n",
            "160398/160398 [==============================] - 2077s 13ms/step - loss: 0.4121 - acc: 0.8556\n",
            "Epoch 9/10\n",
            "160398/160398 [==============================] - 2089s 13ms/step - loss: 0.3420 - acc: 0.8821\n",
            "Epoch 10/10\n",
            "160398/160398 [==============================] - 2088s 13ms/step - loss: 0.2909 - acc: 0.9007\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7af4930438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GilHXUZLYw5g",
        "colab_type": "text"
      },
      "source": [
        "## Model got trained to exactly 90 % Accuaracy, considering the number of model Architectures I have tried, this much Accuracy outcome is feasible, "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LWRgopFanMh",
        "colab_type": "text"
      },
      "source": [
        "## *Accuarcy on Test Set*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bnr4bLgciD0Y",
        "colab_type": "code",
        "outputId": "a0ddd5ab-8eda-4818-902c-5c4600fe1d2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "loss, acc = model.evaluate(X_test_set, Y_test_one_hot)\n",
        "print()\n",
        "print(\"Test accuracy = \", acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "53471/53471 [==============================] - 215s 4ms/step\n",
            "\n",
            "Test accuracy =  0.674702175016327\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B6dK0IHa4wq",
        "colab_type": "text"
      },
      "source": [
        "### For now, can be satisfied with Test Set Accuracy which is close to 70% -- 67.5 % exactly\n",
        "\n",
        "### considering Accuracy is on the data, the model has never seen \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2MPoSQPbsMV",
        "colab_type": "text"
      },
      "source": [
        "## Saving the Model ---\n",
        "## this saves Architecture of the model along with the trained weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XnKkiqjtbonl",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "model.save('/gdrive/My Drive/sentimenet_analyses_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S26UWYtlb7IJ",
        "colab_type": "text"
      },
      "source": [
        "## Load the Trained Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGagVJ3MRI6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drug_rating_model = load_model('/gdrive/My Drive/sentimenet_analyses_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEKC0x5XU1Sb",
        "colab_type": "text"
      },
      "source": [
        "## Preparing for Manual Data Analyses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJxXHHo5cbl3",
        "colab_type": "text"
      },
      "source": [
        "## Manual Data Analyses can hint me on what type of Reviews the model is not performing well, \n",
        "## It can help me in which direction I should spent my efforts to further improve the accuracy "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSiWKjcOVKwv",
        "colab_type": "code",
        "outputId": "9a59a9fe-7a7c-4685-8f62-636601c64346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(10):\n",
        "  print(\"Review >\", test_data['review'][i])\n",
        "  print(\"Original Rating >\", test_data['rating'][i])\n",
        "  test_example = np.array([X_test_set[i]])\n",
        "  \n",
        "  \"\"\"Since Keras prediction only works on Batch Inputs only,\n",
        "      not on single test sample,\n",
        "      explicitly creating a batch of single example\n",
        "  \"\"\"\n",
        "  y_pred = model.predict_classes(test_example)\n",
        "  result = 1 + y_pred[0]\n",
        "  \n",
        "  \"\"\"Readjusting the result by adding 1 to the predicted result\n",
        "     (coz we subtracted 1 before, to account for that)\n",
        "      to get Rating from Lables \"\"\"\n",
        "  \n",
        "  print(\"Predicted Rating >\", result)\n",
        "  print(\"_____________________________________________________________________________________________\")\n",
        "  print(\"\\n\")\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review > \"I&#039;ve tried a few antidepressants over the years (citalopram, fluoxetine, amitriptyline), but none of those helped with my depression, insomnia &amp; anxiety. My doctor suggested and changed me onto 45mg mirtazapine and this medicine has saved my life. Thankfully I have had no side effects especially the most common - weight gain, I&#039;ve actually lost alot of weight. I still have suicidal thoughts but mirtazapine has saved me.\"\n",
            "Original Rating > 10\n",
            "Predicted Rating > 10\n",
            "_____________________________________________________________________________________________\n",
            "\n",
            "\n",
            "Review > \"My son has Crohn&#039;s disease and has done very well on the Asacol.  He has no complaints and shows no side effects.  He has taken as many as nine tablets per day at one time.  I&#039;ve been very happy with the results, reducing his bouts of diarrhea drastically.\"\n",
            "Original Rating > 8\n",
            "Predicted Rating > 8\n",
            "_____________________________________________________________________________________________\n",
            "\n",
            "\n",
            "Review > \"Quick reduction of symptoms\"\n",
            "Original Rating > 9\n",
            "Predicted Rating > 9\n",
            "_____________________________________________________________________________________________\n",
            "\n",
            "\n",
            "Review > \"Contrave combines drugs that were used for alcohol, smoking, and opioid cessation. People lose weight on it because it also helps control over-eating. I have no doubt that most obesity is caused from sugar/carb addiction, which is just as powerful as any drug. I have been taking it for five days, and the good news is, it seems to go to work immediately. I feel hungry before I want food now. I really don&#039;t care to eat; it&#039;s just to fill my stomach. Since I have only been on it a few days, I don&#039;t know if I&#039;ve lost weight (I don&#039;t have a scale), but my clothes do feel a little looser, so maybe a pound or two. I&#039;m hoping that after a few months on this medication, I will develop healthier habits that I can continue without the aid of Contrave.\"\n",
            "Original Rating > 9\n",
            "Predicted Rating > 9\n",
            "_____________________________________________________________________________________________\n",
            "\n",
            "\n",
            "Review > \"I have been on this birth control for one cycle. After reading some of the reviews on this type and similar birth controls I was a bit apprehensive to start. Im giving this birth control a 9 out of 10 as I have not been on it long enough for a 10. So far I love this birth control! My side effects have been so minimal its like Im not even on birth control! I have experienced mild headaches here and there and some nausea but other than that ive been feeling great! I got my period on cue on the third day of the inactive pills and I had no idea it was coming because I had zero pms! My period was very light and I barely had any cramping! I had unprotected sex the first month and obviously didn&#039;t get pregnant so I&#039;m very pleased! Highly recommend\"\n",
            "Original Rating > 9\n",
            "Predicted Rating > 9\n",
            "_____________________________________________________________________________________________\n",
            "\n",
            "\n",
            "Review > \"4 days in on first 2 weeks.  Using on arms and face. Put vaseline on lips, under eyes and in nostrils to protect from cream.  So far no reaction at all.  I know I have many pre cancer and thought I would light up like a Christmas tree but so far so good.  Maybe it&#039;s coming but time will tell.\"\n",
            "Original Rating > 4\n",
            "Predicted Rating > 4\n",
            "_____________________________________________________________________________________________\n",
            "\n",
            "\n",
            "Review > \"I&#039;ve had the copper coil for about 3 months now. I was really excited at the thought of not taking hormones. I&#039;m good with pain however I nearly fainted with insertion, couldn&#039;t belive how painful it was; the doctor did say it is very painful for some. Well 3 months in, my periods last 11 days and I&#039;m in pain for about 15 days with random twangs especially in the left side and I&#039;m considering whether I want to put up with the intense pain and heavy periods. I&#039;d recommend this 100% to somebody who doesn&#039;t already have heavy painful periods but right now it just isn&#039;t for me\"\n",
            "Original Rating > 6\n",
            "Predicted Rating > 2\n",
            "_____________________________________________________________________________________________\n",
            "\n",
            "\n",
            "Review > \"This has been great for me. I&#039;ve been on it for 2 weeks and in the last week I only had 3 headaches which went away with 2 Tylenol. I was having chronic daily headaches that wouldn&#039;t go away no matter what I took. I&#039;m still a little sleepy during the day, but I know that will get better. I take 10mg at night.\"\n",
            "Original Rating > 9\n",
            "Predicted Rating > 9\n",
            "_____________________________________________________________________________________________\n",
            "\n",
            "\n",
            "Review > \"Ive been on Methadone for over ten years and currently,I am trying to get off of this drug. Ive been decreasing my does 2 mgs per month for over a year. I am at 3 mgs and really starting to feel the withdraw.I don&#039;t plan to get my next 30 doses.because its almost rediculous how little it does for me. I have 3 does doses of 3 mg and Im terrified. Can anyone give me some truthful encouragement?.....\"\n",
            "Original Rating > 7\n",
            "Predicted Rating > 6\n",
            "_____________________________________________________________________________________________\n",
            "\n",
            "\n",
            "Review > \"I was on this pill for almost two years. It does work as far as not getting pregnant however my experience at first was it didn&#039;t make a huge difference then 6 or 7 months into it my sex drive went down, along with being very very dry, my moodiness increased drastically. I would cry one second and then get angry with my husband over anything and everything. My skin has gotten a lot worse, I broke out in places I never had in the last week. So now I am on Yaz.\"\n",
            "Original Rating > 2\n",
            "Predicted Rating > 2\n",
            "_____________________________________________________________________________________________\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTaRC21odCfN",
        "colab_type": "text"
      },
      "source": [
        "### Hmmm, To be Honest, After looking at the above results, I am pretty much satisfied with the quality of Rating I am getting from the model"
      ]
    }
  ]
}